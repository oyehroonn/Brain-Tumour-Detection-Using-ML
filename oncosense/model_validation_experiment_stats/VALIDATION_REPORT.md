# OncoSense Model Validation Report

## Executive Summary

**âš ï¸ CRITICAL FINDING: The reported 98.08% model accuracy is INVALID**

Our comprehensive validation experiments have uncovered severe data leakage that artificially inflated the model's reported performance. The DenseNet121 brain tumor classification model trained on the Kaggle 4-Class 7023 Images dataset achieved 98.08% validation accuracy, but this metric is unreliable due to critical data integrity issues.

### Key Findings at a Glance

| Metric | Value | Status |
|--------|-------|--------|
| Claimed Accuracy | 98.08% | âŒ INVALID |
| Testâ†’Train Contamination | 69.6% | ğŸ”´ CRITICAL |
| Images Leaked | 816 / 1,172 | ğŸ”´ CRITICAL |
| Near-Duplicate Leakage | 2.0% | ğŸŸ¡ NOTICE |
| Expected True Accuracy | ~75-85% | ğŸŸ¢ Literature Range |

---

## Background

### The Problem We Investigated

When our DenseNet121 model achieved 98% accuracy on brain tumor classification, skepticism arose: such high performance is atypical for medical imaging tasks on this dataset. Literature typically reports 75-85% accuracy on similar 4-class brain tumor datasets.

We designed 7 validation experiments to investigate whether our accuracy was genuine or artificially inflated.

### Original Dataset Context

The Kaggle dataset "MRI Brain Tumor Dataset (4-Class 7023 Images)" contains:
- **Total Images**: 7,023 (6,597 after exact deduplication)
- **Classes**: Glioma, Meningioma, Pituitary, No Tumor
- **Original Split**: Pre-divided into Train (5,425) and Test (1,172) folders
- **Source**: Compiled from multiple research datasets

---

## Validation Experiments

### Experiment 7: Original Train/Test Split Analysis

**Purpose**: Determine if our random re-splitting caused test data to leak into training.

**Methodology**: 
- Extracted original "Train" or "Test" designation from filenames
- Cross-referenced with our manifest's assigned splits
- Computed contamination rates

**Results**:

```
Cross-Tabulation Matrix:
                    Our Train   Our Val   Our Test
Original Train      3,796       525       1,104
Original Test         816       101         255
```

**Critical Finding**: **69.6% of original test images ended up in our training set**

| Class | Original Test Images | Leaked to Train | Contamination Rate |
|-------|---------------------|-----------------|-------------------|
| Glioma | 299 | 210 | 70.23% |
| Meningioma | 246 | 169 | 68.70% |
| Pituitary | 298 | 205 | 68.79% |
| No Tumor | 329 | 232 | 70.52% |
| **TOTAL** | **1,172** | **816** | **69.62%** |

**Verdict**: ğŸ”´ **CRITICAL** - Severe train/test contamination detected

**Interpretation**: By randomly re-splitting the entire dataset without respecting the original boundaries, we accidentally mixed test images into training. The model then "memorized" these images during training and recognized them during validation, producing artificially high accuracy.

---

### Experiment 6: Near-Duplicate Leakage Check

**Purpose**: Detect perceptually similar images across splits using pHash.

**Methodology**:
- Computed perceptual hash (pHash) for all images
- Calculated Hamming distance between images in different splits
- Threshold: 10 (images with distance â‰¤ 10 considered near-duplicates)

**Results**:

| Leak Type | Count | Rate |
|-----------|-------|------|
| Test â†’ Train | 6 | 2.0% |
| Val â†’ Train | 9 | 1.44% |
| Test â†’ Val | 0 | 0.0% |

**Additional Findings**:
- All 6 test-train leaks had the **same label** (no cross-class confusion)
- Average Hamming distance: 9.7 (very close to threshold)
- Most leaks in "no_tumor" class

**Verdict**: ğŸŸ¡ **NOTICE** - Minor near-duplicate leakage detected

**Interpretation**: While the near-duplicate leakage (2%) is relatively minor compared to the split contamination (69.6%), it represents an additional source of data leakage. The `quick_setup.py` script assigned unique cluster IDs to each image, effectively bypassing the pHash clustering designed to prevent this.

---

### Experiment 5: Grad-CAM Analysis

**Purpose**: Visualize what image regions the model focuses on for predictions.

**Status**: âš ï¸ **ERROR** - Module `skimage` not installed

**Expected Analysis**: Would have shown whether the model attends to:
- Actual tumor regions (good)
- Image borders, corners, or artifacts (bad - "Clever Hans" effect)

**Recommendation**: Install `scikit-image` and re-run to validate model attention.

---

### Experiments 1-4: Pending

| Experiment | Name | Purpose | Status |
|------------|------|---------|--------|
| 4 | Proper Clustering | Re-split with correct pHash clustering | â³ Pending |
| 3 | External Validation | Test on Figshare dataset | â³ Pending |
| 2 | Center Crop Test | Detect learning from center region only | â³ Pending |
| 1 | Random Label Test | Detect memorization via random labels | â³ Pending |

---

## Training Metrics (Now Invalidated)

The following metrics were recorded during training but are **no longer valid** due to data leakage:

| Epoch | Train Loss | Train Acc | Train F1 | Val Loss | Val Acc | Val F1 |
|-------|-----------|-----------|----------|----------|---------|--------|
| 1 | 0.2449 | 80.10% | 0.7970 | 0.0928 | 92.01% | 0.9178 |
| 2 | 0.0950 | 91.47% | 0.9130 | 0.0749 | 94.25% | 0.9424 |
| 3 | 0.0752 | 93.38% | 0.9325 | 0.0487 | 95.53% | 0.9539 |
| 4 | 0.0513 | 95.42% | 0.9533 | 0.0472 | 96.17% | 0.9611 |
| 5 | 0.0512 | 95.46% | 0.9536 | 0.0572 | 96.33% | 0.9622 |
| 6 | 0.0384 | 96.20% | 0.9613 | 0.0429 | **98.08%** | 0.9802 |

**Why These Metrics Are Invalid**:
1. 69.6% of validation images were seen during training
2. The model memorized test data rather than learning generalizable features
3. True generalization performance is unknown

---

## Visualizations Guide

The following visualizations have been generated to illustrate these findings:

### 1. Executive Dashboard (`01_executive_dashboard.png`)
High-level overview with KPI cards, contamination gauge, and experiment status grid.

### 2. Data Flow Diagram (`02_data_flow_contamination.png`)
Sankey-style visualization showing how images flowed from original splits to our splits, highlighting the 816 leaked test images.

### 3. Contamination Heatmap (`03_contamination_heatmap.png`)
Cross-tabulation matrix with color intensity showing the severity of contamination.

### 4. Per-Class Contamination (`04_perclass_contamination.png`)
Bar charts showing contamination is uniform (~70%) across all four tumor classes.

### 5. Annotated Training Curves (`05_training_curves_annotated.png`)
Original training curves with "CONTAMINATED DATA" watermark and expected accuracy range highlighted.

### 6. Leakage Analysis (`06_leakage_analysis.png`)
Four-panel analysis of near-duplicate leakage including counts, rates, and summary statistics.

### 7. Experiment Summary Grid (`07_experiment_summary_grid.png`)
Visual cards showing status of all 7 validation experiments.

### 8. Accuracy Comparison (`08_accuracy_comparison.png`)
Side-by-side comparison of claimed (98%) vs expected (~80%) accuracy with pie chart of data composition.

### 9. Validation Timeline (`09_validation_timeline.png`)
Storytelling timeline showing how the data leakage occurred step by step.

### 10. Final Verdict Infographic (`10_final_verdict_infographic.png`)
Comprehensive one-page summary with key metrics, evidence, and recommendations.

---

## Data Tables (CSV)

### `validation_metrics.csv`
All validation metrics with their thresholds and status classifications.

### `per_class_contamination.csv`
Per-class breakdown of test-to-train contamination rates.

### `split_cross_tabulation.csv`
Complete cross-tabulation of original vs our splits.

### `experiment_verdicts.csv`
Summary of all experiment outcomes and key findings.

### `training_metrics_per_epoch.csv`
Full training history with validity annotations.

### `near_duplicate_leaks.csv`
Detailed list of near-duplicate pairs found across splits.

---

## Root Cause Analysis

### What Went Wrong

1. **Improper Re-Splitting**: The Kaggle dataset came with its own train/test split. Our `quick_setup.py` script ignored these boundaries and randomly re-split all images.

2. **Bypassed pHash Clustering**: Instead of using proper perceptual hash clustering to keep near-duplicates together, the script assigned unique `cluster_id` to each image.

3. **No Patient-Level Information**: The dataset lacks patient IDs, making it impossible to ensure slices from the same scan stay in the same split.

### The Consequences

```
Original Dataset                    Our Processing                   Result
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Train: 5,425   â”‚  Random Split   â”‚ Train: 4,612   â”‚              â”‚ 816 TEST imagesâ”‚
â”‚ Test:  1,172   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º   â”‚ Val:     626   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â–º    â”‚ leaked to TRAINâ”‚
â”‚                â”‚                 â”‚ Test:  1,359   â”‚              â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                   69.6% contamination!
```

---

## Recommendations

### Immediate Actions

1. **Respect Original Splits**: Re-process data keeping original train images for training only, and original test images for validation/testing only.

2. **Implement Proper Clustering**: Use the `proper_clustering.py` experiment to re-split with correct pHash deduplication.

3. **Install Missing Dependencies**: Install `scikit-image` to run Grad-CAM analysis.

### Validation Actions

4. **Run Remaining Experiments**: Execute experiments 1-4 to complete the validation suite:
   - Random Label Test (detect memorization)
   - Center Crop Test (detect spurious patterns)
   - External Validation (true generalization test)
   - Proper Clustering Retrain (valid accuracy)

5. **External Dataset Validation**: Test on Figshare Jun Cheng dataset for true generalization assessment.

### Long-Term Improvements

6. **Seek Patient-Level Metadata**: If available, use patient IDs to prevent slice leakage.

7. **Implement Cross-Validation**: Use stratified k-fold cross-validation with proper grouping.

8. **Calibration Analysis**: Add uncertainty quantification and calibration metrics.

---

## Conclusion

**The 98.08% accuracy claim is invalid.**

Our validation experiments have definitively shown that severe data leakage occurred during preprocessing. Approximately 70% of the original test images ended up in our training set, allowing the model to memorize rather than generalize.

The true performance of the model is unknown but likely falls in the 75-85% range typical for this task. To determine actual performance, the model must be retrained on properly split data and validated on held-out or external datasets.

This validation exercise demonstrates the critical importance of:
- Respecting original dataset boundaries
- Implementing proper deduplication
- Validating claims with rigorous experiments

---

## Appendix: File Listing

```
model_validation_experiment_stats/
â”œâ”€â”€ VALIDATION_REPORT.md              # This document
â”œâ”€â”€ generate_validation_visualizations.py  # Visualization generator script
â”‚
â”œâ”€â”€ Visualizations (PNG)
â”‚   â”œâ”€â”€ 01_executive_dashboard.png
â”‚   â”œâ”€â”€ 02_data_flow_contamination.png
â”‚   â”œâ”€â”€ 03_contamination_heatmap.png
â”‚   â”œâ”€â”€ 04_perclass_contamination.png
â”‚   â”œâ”€â”€ 05_training_curves_annotated.png
â”‚   â”œâ”€â”€ 06_leakage_analysis.png
â”‚   â”œâ”€â”€ 07_experiment_summary_grid.png
â”‚   â”œâ”€â”€ 08_accuracy_comparison.png
â”‚   â”œâ”€â”€ 09_validation_timeline.png
â”‚   â””â”€â”€ 10_final_verdict_infographic.png
â”‚
â””â”€â”€ Data Tables (CSV)
    â”œâ”€â”€ validation_metrics.csv
    â”œâ”€â”€ per_class_contamination.csv
    â”œâ”€â”€ split_cross_tabulation.csv
    â”œâ”€â”€ experiment_verdicts.csv
    â”œâ”€â”€ training_metrics_per_epoch.csv
    â””â”€â”€ near_duplicate_leaks.csv
```

---

*Generated: February 2026 | OncoSense Model Validation Suite v1.0*
